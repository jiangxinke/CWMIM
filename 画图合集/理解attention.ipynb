{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 邻接矩阵和输入\n",
    "\n",
    "首先构建邻接矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\ManifoldFlow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "E:\\ProgramData\\Anaconda3\\envs\\ManifoldFlow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "E:\\ProgramData\\Anaconda3\\envs\\ManifoldFlow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "E:\\ProgramData\\Anaconda3\\envs\\ManifoldFlow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "E:\\ProgramData\\Anaconda3\\envs\\ManifoldFlow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "E:\\ProgramData\\Anaconda3\\envs\\ManifoldFlow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import lib.toy_data as toy_data\n",
    "from lib.toy_data import generate_slope\n",
    "import lib.utils as utils\n",
    "from lib.utils import standard_normal_logprob, set_random_seed, standard_uniform_logprob, x2z\n",
    "from lib.utils import count_nfe, count_total_time\n",
    "from lib.utils import build_model_tabular, evaluation\n",
    "from lib.visualize_flow import visualize_transform, standard_fig_save\n",
    "import lib.layers.odefunc as odefunc\n",
    "from lib.layers.container import EpsGenerator, MyModel, GNN\n",
    "from args import add_args\n",
    "import numpy as np\n",
    "# from tensorboardX import SummaryWriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from lib.dataloader import load_features_labels, MyDataSet, load_loc_data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "set_random_seed(1024)\n",
    "device = torch.device('cuda:' + str(0) if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从z计算pair wise的距离，并生成邻接矩阵\n",
    "def normed_A(z):\n",
    "    # z 是N*3的原始坐标，varrho是限制，生成可以直接使用的A和保留比例\n",
    "    Dis = torch.cdist(z, z, p=2).float()  # distance matrix\n",
    "    m = Dis.mean()\n",
    "\n",
    "    # 不带权A\n",
    "    A = torch.eye(Dis.size(0), dtype=torch.float32)\n",
    "    # A[Dis < self.varrho] = 1\n",
    "    A[Dis < m] = 1\n",
    "    remain_persent = (A.sum() / (A.shape[0] * A.shape[1])).item()\n",
    "    print(\"Sparsity of A(0 is empty):\", remain_persent * 100, \"%\")\n",
    "    print(\"Cut persent:\", 100 - remain_persent * 100, \"%\")\n",
    "    # 带权A\n",
    "    # A = torch.where(Dis < m, Dis, torch.eye(Dis.size(0), dtype=torch.float32)).float()\n",
    "    # degree matrix\n",
    "    degree = A.sum(1)\n",
    "    D = torch.diag(torch.pow(degree, -0.5))\n",
    "    normed_A = D.mm(A).mm(D)\n",
    "    return normed_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of D:\\projects\\SF\\toy_example\\data\\DDH_right.csv: (21, 2164) \n"
     ]
    }
   ],
   "source": [
    "data = \"DDH_right\"\n",
    "seq_len = 3\n",
    "pre_len = 1\n",
    "batch_size = 5\n",
    "feature_data_path = r\"D:\\projects\\SF\\toy_example\\data\\{}.csv\".format(data)\n",
    "\n",
    "# 坐标数据\n",
    "loc_data = np.loadtxt(fname=feature_data_path, delimiter=\",\", skiprows=1)[:, 0:3]\n",
    "loc_data = torch.tensor(loc_data)\n",
    "# normalize\n",
    "for i in range(3):\n",
    "    _min = loc_data[:, i].min()\n",
    "    _max = loc_data[:, i].max()\n",
    "    loc_data[:, i] = (loc_data[:, i] - _min)/(_max - _min)\n",
    "\n",
    "N = loc_data.shape[0]\n",
    "\n",
    "# 特征loader\n",
    "dataset = MyDataSet(feature_data_path, seq_len, pre_len, device)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "feature_iter = iter(train_loader)\n",
    "num_nodes = dataset.num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of A(0 is empty): 49.33028519153595 %\n",
      "Cut persent: 50.66971480846405 %\n"
     ]
    }
   ],
   "source": [
    "adj = normed_A(loc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 图注意力层的定义\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_c = in_c\n",
    "        self.out_c = out_c\n",
    "\n",
    "        self.F = F.softmax\n",
    "\n",
    "        self.W = nn.Linear(in_c, out_c, bias=False)  # y = W * x\n",
    "        self.b = nn.Parameter(torch.Tensor(out_c))\n",
    "\n",
    "        nn.init.normal_(self.W.weight)\n",
    "        nn.init.normal_(self.b)\n",
    "\n",
    "    def forward(self, inputs, graph):\n",
    "        \"\"\"\n",
    "        :param inputs: input features, [B, N, C].\n",
    "        :param graph: graph structure, [N, N].\n",
    "        :return:\n",
    "            output features, [B, N, D].\n",
    "        \"\"\"\n",
    "\n",
    "        # eq.1 计算注意力系数e_ij\n",
    "        h = self.W(inputs)  # [B, N, D] = [B, N, C] * [C, D]\n",
    "        outputs = torch.bmm(h, h.transpose(1, 2)) * graph.unsqueeze(0)  # [B, N, N]      x(i)^T * x(j)\n",
    "        outputs.data.masked_fill_(torch.eq(outputs, 0), -float(1e16))   # x(i)|| x(j)\n",
    "\n",
    "        # 注意力系数归一化\n",
    "        attention = self.F(outputs, dim=2)   # [B, N, N]\n",
    "        return torch.bmm(attention, h) + self.b  # [B, N, N] * [B, N, D]\n",
    "\n",
    "\n",
    "class GATSubNet(nn.Module):\n",
    "    def __init__(self, in_c, hid_c, out_c, n_heads):\n",
    "        super(GATSubNet, self).__init__()\n",
    "\n",
    "        self.attention_module = nn.ModuleList([GraphAttentionLayer(in_c, hid_c) for _ in range(n_heads)])\n",
    "        self.out_att = GraphAttentionLayer(hid_c * n_heads, out_c)\n",
    "\n",
    "        self.act = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, inputs, graph):\n",
    "        \"\"\"\n",
    "        :param inputs: [B, N, C]\n",
    "        :param graph: [N, N]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 将不同的head拼接到一起\n",
    "        outputs = torch.cat([attn(inputs, graph) for attn in self.attention_module], dim=-1)  # [B, N, hid_c * h_head]\n",
    "        outputs = self.act(outputs)\n",
    "        # eq.6 聚合，原文是求平均，这里是一个额外的attention层\n",
    "        outputs = self.out_att(outputs, graph)\n",
    "\n",
    "        return self.act(outputs)\n",
    "\n",
    "\n",
    "# GAT 网络的定义\n",
    "class GATNet(nn.Module):\n",
    "    def __init__(self, in_c, hid_c, out_c, n_heads):\n",
    "        super(GATNet, self).__init__()\n",
    "        self.subnet = GATSubNet(in_c, hid_c, out_c, n_heads)\n",
    "\n",
    "    def forward(self, data, graph, device):\n",
    "        graph = graph.to(device)  # [N, N]\n",
    "        flow = data.to(device)  # [B, N, T, C]\n",
    "\n",
    "        B, N = flow.size(0), flow.size(1)\n",
    "        flow = flow.view(B, N, -1)  # [B, N, T * C]\n",
    "\n",
    "        # prediction = self.subnet(flow, graph).unsqueeze(2)  # [B, N, 1, C]\n",
    "        prediction = self.subnet(flow, graph)  # [B, N, C]\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat = GATNet(in_c=seq_len, hid_c=3, out_c=pre_len, n_heads=2).to(device)\n",
    "\n",
    "loss = torch.nn.MSELoss(reduce=None, size_average=None)\n",
    "optimizer = optim.Adam(gat.parameters(), lr=0.001, weight_decay=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.3694071769714355\n",
      "6.177985668182373\n",
      "5.815783977508545\n",
      "5.885026931762695\n",
      "5.625741481781006\n"
     ]
    }
   ],
   "source": [
    "for itr in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    torch.cuda.empty_cache()\n",
    "    try:\n",
    "        x, y = feature_iter.next()\n",
    "    except StopIteration:\n",
    "        feature_iter = iter(train_loader)\n",
    "        x, y = feature_iter.next()\n",
    "    # x = [B, N, seq_len], y = [B, N, pre_len]\n",
    "    pre = gat(x, adj, device)\n",
    "    l = loss(input=pre, target=y)\n",
    "\n",
    "    print(l.item())\n",
    "\n",
    "    l.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = feature_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = gat(x, adj, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2164, 2164])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.8491],\n",
       "         [2.8503],\n",
       "         [2.8510],\n",
       "         ...,\n",
       "         [2.8544],\n",
       "         [2.8541],\n",
       "         [2.8509]],\n",
       "\n",
       "        [[3.0372],\n",
       "         [3.0553],\n",
       "         [3.0650],\n",
       "         ...,\n",
       "         [2.9490],\n",
       "         [2.9405],\n",
       "         [2.9727]],\n",
       "\n",
       "        [[2.6847],\n",
       "         [2.6657],\n",
       "         [2.6554],\n",
       "         ...,\n",
       "         [2.7433],\n",
       "         [2.7463],\n",
       "         [2.7536]]], device='cuda:0', grad_fn=<LeakyReluBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "956f6b8982f9276e923cc4a0a568795cb8690031d56c97c3480dd015765dde0e"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
